ollama:
  host: http://localhost:11434
  # CPU-optimized models (8-core, 16GB RAM, no GPU)
  # Strategy: Fast, efficient models that work well on CPU
  model_qa: phi3:mini              # Fast CPU model for Q&A (2.3GB)
  model_summary: gemma2:2b         # Efficient for summarization (1.6GB)
  model_verification: phi3:mini    # Fast verification (2.3GB)
  model_extraction: qwen2.5:7b     # Good balance for CPU (4.7GB)
  model_embedding: nomic-embed-text
  
embeddings:
  # Choose embedding backend: 'ollama' or 'sentence-transformers'
  # WHY sentence-transformers: 2-3x faster on CPU, runs locally, no Ollama needed
  backend: "sentence-transformers"  # Options: 'ollama' or 'sentence-transformers'
  model: "sentence-transformers/all-MiniLM-L6-v2"  # Fast, quality embeddings (80MB)
  # all-MiniLM-L6-v2: 384 dimensions, ~200 chunks/sec on CPU, great for technical text
  # Timeouts optimized for CPU
  timeout: 600        # Q&A (10 min) - increased for stability
  timeout_summary: 600   # Summary (10 min) - no word limit, needs more time
  timeout_extraction: 900  # Extraction (15 min)
  timeout_verification: 300  # Verification (5 min)
  
vector_db:
  type: chromadb
  path: ./chroma_db
  distance_metric: cosine
  
chunking:
  # Single strategy chunking for narrative text only
  # Tables are stored complete in database, not chunked
  
  # Fine-grained chunks for all queries (Q&A and Summary)
  fine_grained:
    chunk_size: 500      # 500 words for specific details
    chunk_overlap: 150   # 150 words overlap
    method: "recursive"  # RecursiveCharacterTextSplitter for natural breaks
    clean_headers: true  # Remove "Page X of Y" footers
    exclude_tables: true # Tables are handled separately in database
    
extraction:
  enable_llm_fallback: true
  confidence_threshold: 0.7
  timeout: 300  # 5 minutes for complex extractions
  
validation:
  # Physical constraints
  md_tvd_tolerance: 1.0
  inclination_max: 90
  temperature_gradient_min: 20  # °C/km
  temperature_gradient_max: 40  # °C/km
  pressure_gradient_min: 9      # kPa/m
  pressure_gradient_max: 12     # kPa/m
  well_depth_min: 500           # meters
  well_depth_max: 5000          # meters
  # Physical validation ranges (all pipe IDs in inches)
  min_pipe_id: 2.0      # inches (minimum realistic pipe ID)
  max_pipe_id: 30.0     # inches (maximum realistic pipe ID)
  max_md: 5000.0        # meters (maximum measured depth)
  max_tvd: 5000.0       # meters (maximum true vertical depth)
  # Fact verification thresholds
  min_support_rate: 0.8   # 80% of claims must be supported
  min_confidence: 0.7     # 70% confidence threshold
  # Confidence scoring
  high_confidence: 0.85   # High confidence threshold
  low_confidence: 0.50    # Low confidence threshold
  # Completeness requirements
  require_trajectory: true
  require_casing: true
  require_tubing: false
  require_pvt: false
  # User interaction
  always_warn_users: true  # Always warn about validation issues
  always_ask_confirmation: true  # Always ask for confirmation before analysis
  
retrieval:
  hybrid_weight_dense: 0.8   # Semantic search for technical content
  hybrid_weight_sparse: 0.2  # Keyword matching weight
  # Hybrid retrieval: Fetch both table summaries and narrative chunks
  top_k_qa: 15           # Focused retrieval for Q&A
  top_k_extraction: 30   # Include table rows + surrounding context
  top_k_summary: 10      # Section-level summaries
  top_k_tables: 20       # Dedicated retrieval for table rows
  top_k_fine: 15         # Precise matches
  top_k_coarse: 8        # High-level context
  prioritize_tables: true  # Boost table results for numerical queries (pipe ID, depths, etc.)
  
summarization:
  default_words: null  # No default limit - generate as much as needed
  tolerance_percent: 20  # ±20% tolerance for word count (flexible)
  max_retries: 1  # Retry once if specific word count requested
  enable_citations: true  # Include source citations in summaries
  enable_verification: true  # Perform fact verification on summaries
  min_chunks_for_verification: 5  # Minimum chunks needed for verification
  # WHY no limit: User said "write as much as needed"

ui:
  port: 7860
  share: false
  server_name: "0.0.0.0"
