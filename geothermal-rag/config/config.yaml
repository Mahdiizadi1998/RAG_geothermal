ollama:
  host: http://localhost:11434
  # Advanced Agentic RAG models - following reference architecture
  # llama3.1:8b - Main reasoning engine (fast, capable, 4.7GB)
  # llava:7b - Vision-language model for image/plot processing (4.7GB)
  model_qa: llama3.1:8b           # Primary reasoning & generation (4.7GB)
  model_summary: llama3.1:8b      # Same model for consistency
  model_verification: llama3.1:8b # Same model for verification
  model_extraction: llama3.1:8b   # Same model for extraction
  model_vision: llava:7b          # Vision-language model for images (4.7GB)
  model_embedding: nomic-embed-text  # Fast embeddings
  
embeddings:
  # Using sentence-transformers with all-MiniLM-L6-v2 for all components
  # WHY: 2-3x faster on CPU, runs locally, no Ollama needed, consistent across all modules
  backend: "sentence-transformers"  # Options: 'ollama' or 'sentence-transformers'
  model: "all-MiniLM-L6-v2"  # Fast, quality embeddings (80MB, 384 dims)
  # all-MiniLM-L6-v2: 384 dimensions, ~200 chunks/sec on CPU, great for technical text

# Advanced Semantic Chunking (Ultimate Semantic Chunker)
semantic_chunking:
  enabled: true
  similarity_threshold: 0.7  # Threshold for semantic breakpoints
  min_chunk_size: 200        # Minimum words per chunk
  max_chunk_size: 800        # Maximum words per chunk
  context_window: 100        # Contextual enrichment window
  use_late_chunking: true    # Enable Jina AI-style late chunking
  use_contextual_enrichment: true  # Enable Anthropic-style context prepending

# RAPTOR Tree Configuration
raptor:
  enabled: true
  min_cluster_size: 5        # Minimum cluster size for HDBSCAN
  max_tree_height: 3         # Maximum tree depth (0=leaves, 1+=summaries)
  summary_word_count: 300    # Words per cluster summary
  
# Knowledge Graph Configuration  
knowledge_graph:
  enabled: true
  similarity_threshold: 0.7  # Minimum similarity for edges
  metadata_edge_types:       # Types of metadata connections
    - same_well
    - same_operator
    - same_formation
  max_neighbors: 10          # Max neighbors per node in similarity edges

# BM25 Sparse Retrieval Configuration
bm25:
  enabled: true
  k1: 1.5                    # Term frequency saturation
  b: 0.75                    # Length normalization
  
# Reranking Configuration
reranking:
  enabled: true
  method: "cross-encoder"    # Options: 'cross-encoder', 'llm', 'none'
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Cross-encoder model
  rrf_k: 60                  # Reciprocal Rank Fusion constant

# Vision Processing Configuration
vision:
  enabled: true
  model: "llava:7b"          # Vision-language model
  classify_images: true      # Classify image types before captioning
  max_caption_length: 300    # Maximum caption tokens
  # Timeouts optimized for CPU
  timeout: 600        # Q&A (10 min) - increased for stability
  timeout_summary: 600   # Summary (10 min) - no word limit, needs more time
  timeout_extraction: 900  # Extraction (15 min)
  timeout_verification: 300  # Verification (5 min)
  
vector_db:
  type: chromadb
  path: ./chroma_db
  distance_metric: cosine
  
chunking:
  # Single strategy chunking for narrative text only
  # Tables are stored complete in database, not chunked
  
  # Fine-grained chunks for all queries (Q&A and Summary)
  fine_grained:
    chunk_size: 500      # 500 words for specific details
    chunk_overlap: 150   # 150 words overlap
    method: "recursive"  # RecursiveCharacterTextSplitter for natural breaks
    clean_headers: true  # Remove "Page X of Y" footers
    exclude_tables: true # Tables are handled separately in database
    
extraction:
  enable_llm_fallback: true
  confidence_threshold: 0.7
  timeout: 300  # 5 minutes for complex extractions
  
validation:
  # Physical constraints
  md_tvd_tolerance: 1.0
  inclination_max: 90
  temperature_gradient_min: 20  # °C/km
  temperature_gradient_max: 40  # °C/km
  pressure_gradient_min: 9      # kPa/m
  pressure_gradient_max: 12     # kPa/m
  well_depth_min: 500           # meters
  well_depth_max: 5000          # meters
  # Physical validation ranges (all pipe IDs in inches)
  min_pipe_id: 2.0      # inches (minimum realistic pipe ID)
  max_pipe_id: 30.0     # inches (maximum realistic pipe ID)
  max_md: 5000.0        # meters (maximum measured depth)
  max_tvd: 5000.0       # meters (maximum true vertical depth)
  # Fact verification thresholds
  min_support_rate: 0.8   # 80% of claims must be supported
  min_confidence: 0.7     # 70% confidence threshold
  # Confidence scoring
  high_confidence: 0.85   # High confidence threshold
  low_confidence: 0.50    # Low confidence threshold
  # Completeness requirements
  require_trajectory: true
  require_casing: true
  require_tubing: false
  require_pvt: false
  # User interaction
  always_warn_users: true  # Always warn about validation issues
  always_ask_confirmation: true  # Always ask for confirmation before analysis
  
retrieval:
  hybrid_weight_dense: 0.8   # Semantic search for technical content
  hybrid_weight_sparse: 0.2  # Keyword matching weight
  # Hybrid retrieval: Fetch both table summaries and narrative chunks
  top_k_qa: 15           # Focused retrieval for Q&A
  top_k_extraction: 30   # Include table rows + surrounding context
  top_k_summary: 10      # Section-level summaries
  top_k_tables: 20       # Dedicated retrieval for table rows
  top_k_fine: 15         # Precise matches
  top_k_coarse: 8        # High-level context
  prioritize_tables: true  # Boost table results for numerical queries (pipe ID, depths, etc.)
  
summarization:
  default_words: null  # No default limit - generate as much as needed
  tolerance_percent: 20  # ±20% tolerance for word count (flexible)
  max_retries: 1  # Retry once if specific word count requested
  enable_citations: true  # Include source citations in summaries
  enable_verification: true  # Perform fact verification on summaries
  min_chunks_for_verification: 5  # Minimum chunks needed for verification
  # WHY no limit: User said "write as much as needed"

ui:
  port: 7860
  share: false
  server_name: "0.0.0.0"
